{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2d71d4-901a-4a24-8044-f37b0251e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotchaos.syntheticSignals as sp\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import TimedAnimation\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from astropy.timeseries import LombScargle\n",
    "from scipy.integrate import RK45, DOP853\n",
    "from scipy.stats import chi2, iqr\n",
    "from scipy.signal import argrelextrema, savgol_filter, find_peaks_cwt\n",
    "from scipy.spatial.distance import chebyshev\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from pytisean import tiseano, tiseanio\n",
    "\n",
    "from sklearn.neighbors import KDTree, BallTree\n",
    "from scipy.spatial import KDTree as scipyKDTree\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a002f94c-ccec-44eb-a541-da15f9fb4cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Time series:\n",
    "# 0. White noise\n",
    "# 1. stationary GP time series\n",
    "# 2. Simple periodic\n",
    "# 3. KB88 R(t)\n",
    "# 4. Rossler x, y, z\n",
    "# 5. Transformed rossler u, v, w\n",
    "# 6. Lorenz x, y, z\n",
    "\n",
    "# and for each we have 3 baselines: TESS (perfect = 257062 data points), Kepler (perfect = 71422 data points), SPECULOOS (perfect = 44119 data points)\n",
    "#                   and 4 versions: perfect, gapless+noisy, gappy+noiseless, realistic\n",
    "# yielding 13*3*4 = 156 time series to analyze altogether. (But only 78 files, because the perfect and noisy versions are in the same file.)\n",
    "\n",
    "# Need to choose:\n",
    "# 1. time delay (Fraser & Swinney 1986)\n",
    "# 2. embedding dimension (Cao 1997)\n",
    "# and then calculate\n",
    "# 1. correlation dimension (Kurths & Herzel 1987; investigate pytisean too. Look into Thelier window.)\n",
    "# 2. Lyapunov spectrum (Wolf et al. 1985)\n",
    "# 3. Lyapunov dimension (Kaplan-Yorke conjecture; see Eckmann & Ruelle 1985 eq 4.11).\n",
    "\n",
    "# Expected results:\n",
    "# 0. White noise should have no good choice of time delay; mutual info as a function of delay time should be flat.\n",
    "# 1. Not sure!\n",
    "# 2. Simple periodic and KB88 R(t) results should be similar to KB88 figures 9 and 10.\n",
    "# 3. same\n",
    "# 4. Rossler x, y, and z should yield time delay ~ 1/4*rossler_qp and embedding dimension of 3. Rossler z results should be worse than x and y.\n",
    "# 5. Transformed Rossler u, v, and w should yield time delay ~1/4*transformed_rossler_qp and embedding dimension of 3. Variables should be about equally good.\n",
    "# 6. Lorenz x, y, and z should yield time delay ~1/4*lorenz_qp and embedding dimension of 3. z dimension will be insensitive to wing symmetry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813c4bcc-2b1b-4c51-90a3-60ba5f8fb174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folderpaths = ['0_gaussian_noise',\n",
    "               '1_gaussian_process',\n",
    "               '2_simple_periodic',\n",
    "               '3_KB88_r',\n",
    "               '4_rossler',\n",
    "               '5_transformed_rossler',\n",
    "               '6_lorenz']\n",
    "\n",
    "list_of_tsps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4315bc06-ccd0-4094-a5f2-f12dc1427f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class timeSeriesParams(object):\n",
    "    def __init__(self, filename, filepath, **kwargs):\n",
    "        self.filename = filename\n",
    "        self.filepath = filepath\n",
    "        self.QPmethod = None\n",
    "        self.QP = None\n",
    "        self.noisy_QP = None\n",
    "        self.bestTauIdx = None\n",
    "        self.noisy_bestTauIdx = None\n",
    "        self.sat_m = None\n",
    "        self.noisy_sat_m = None\n",
    "\n",
    "        allowed_keys = [\"QPmethod\",\"QP\",\"noisy_QP\",\"bestTauIdx\",\"noisy_bestTauIdx\",\"sat_m\",\"noisy_sat_m\"]\n",
    "\t\t\n",
    "        self.__dict__.update((k,v) for k,v in kwargs.items() if k in allowed_keys)\n",
    "\t\t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7ec96f-1943-4ecf-9f6f-0703b2994034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_gaussianNoise_speculoos_perfect.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     noisy_mutInfo, noisy_bestTauIdx, noisy_qp \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mFS86(time \u001b[38;5;241m=\u001b[39m data[:,\u001b[38;5;241m0\u001b[39m], timeSeries \u001b[38;5;241m=\u001b[39m noisy_ts, QPmethod\u001b[38;5;241m=\u001b[39mQPmethod, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_or_second_local_min\u001b[39m\u001b[38;5;124m\"\u001b[39m, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# choose embedding dimension (Cao 1997)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m E1, E2, sat_m \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcao97\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeSeries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbestTauIdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmMax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m noisy_E1, noisy_E2, noisy_sat_m \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcao97(timeSeries\u001b[38;5;241m=\u001b[39mnoisy_ts, tau\u001b[38;5;241m=\u001b[39mnoisy_bestTauIdx, mMax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     36\u001b[0m tspName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtsp_\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/astro/starspot_chaos/spotchaos/spotchaos/syntheticSignals.pyx:667\u001b[0m, in \u001b[0;36mspotchaos.syntheticSignals.cao97\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/astro/starspot_chaos/spotchaos/spotchaos/syntheticSignals.pyx:736\u001b[0m, in \u001b[0;36mspotchaos.syntheticSignals.cao97\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/scipy/spatial/distance.py:1039\u001b[0m, in \u001b[0;36mchebyshev\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mdot(delta, VI), delta)\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(m)\n\u001b[0;32m-> 1039\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchebyshev\u001b[39m(u, v, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m    Compute the Chebyshev distance.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m     u \u001b[38;5;241m=\u001b[39m _validate_vector(u)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make a dictionary so we don't have to keep rerunning the FS86 and cao97 steps.\n",
    "for folder in folderpaths:\n",
    "#for folder in ['6_lorenz','blah']:\n",
    "    files = os.scandir(\"../data/{0}/\".format(folder))\n",
    "    for file in files:\n",
    "        if \".txt\" in file.name and \"perfect\" in file.name and \"speculoos\" in file.name:\n",
    "            print(file.name)\n",
    "            data = np.genfromtxt(file.path)\n",
    "            \n",
    "            if \"0\" in file.name:\n",
    "                ts = data[:,2]\n",
    "                noisy_ts = data[:,2]\n",
    "            else:\n",
    "                ts = data[:,1]\n",
    "                noisy_ts = data[:,2]\n",
    "                \n",
    "            if \"lorenz\" in file.name and \"_z_\" not in file.name:\n",
    "                QPmethod = \"localMaxSep\"\n",
    "            elif \"0\" in file.name:\n",
    "                QPmethod = \"localMaxSep\"\n",
    "            else:\n",
    "                QPmethod = \"power\"\n",
    "\n",
    "            # choose time delay (Fraser & Swinney 1986)\n",
    "            if \"0\" in file.name:\n",
    "                mutInfo, bestTauIdx, qp = sp.FS86(time = data[:,0], timeSeries = ts, QPmethod=QPmethod, method=\"global_min\", plot=False)\n",
    "                noisy_mutInfo, noisy_bestTauIdx, noisy_qp = sp.FS86(time = data[:,0], timeSeries = noisy_ts, QPmethod=QPmethod, method=\"global_min\", plot=False)\n",
    "            else:\n",
    "                mutInfo, bestTauIdx, qp = sp.FS86(time = data[:,0], timeSeries = ts, QPmethod=QPmethod, method=\"first_or_second_local_min\", plot=False)\n",
    "                noisy_mutInfo, noisy_bestTauIdx, noisy_qp = sp.FS86(time = data[:,0], timeSeries = noisy_ts, QPmethod=QPmethod, method=\"first_or_second_local_min\", plot=False)\n",
    "            \n",
    "            # choose embedding dimension (Cao 1997)\n",
    "            E1, E2, sat_m = sp.cao97(timeSeries=ts, tau=bestTauIdx, mMax=8)\n",
    "            noisy_E1, noisy_E2, noisy_sat_m = sp.cao97(timeSeries=noisy_ts, tau=noisy_bestTauIdx, mMax=8)\n",
    "            \n",
    "            tspName = \"tsp_{0}\".format(file.name.split(\".txt\")[0])\n",
    "            print(tspName)\n",
    "            exec(\"{0} = timeSeriesParams(filename='{1}',filepath='{2}',QPmethod='{3}',QP={4},noisy_QP={5},bestTauIdx={6},noisy_bestTauIdx={7},sat_m={8},noisy_sat_m={9})\".format(tspName,file.name,file.path,QPmethod,qp,noisy_qp,bestTauIdx,noisy_bestTauIdx,sat_m,noisy_sat_m))\n",
    "\n",
    "            exec(\"list_of_tsps.append({0})\".format(tspName))\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8da6d8-95ee-41aa-82ec-5e733189b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_of_tsps)\n",
    "#print(list_of_tsps[0])\n",
    "#print(type(list_of_tsps[0]))\n",
    "#print(type(tsp_0_gaussianNoise_speculoos_perfect))\n",
    "for tsp in list_of_tsps:\n",
    "    with open('{0}.pickle'.format(tsp.filename.split(\".txt\")[0]), 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(tsp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468e3c6c-f709-4f24-9ec6-e4e6909f3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tsps = []\n",
    "\n",
    "files = os.scandir(\".\")\n",
    "for file in files:\n",
    "    if \".pickle\" in file.name:\n",
    "        tspname = \"tsp_{0}\".format(file.name.split(\".pickle\")[0])\n",
    "        with open(file.name, 'rb') as f:\n",
    "            exec(\"tsp_{0} = pickle.load(f)\".format(file.name.split(\".pickle\")[0]))\n",
    "        exec(\"list_of_tsps.append(tsp_{0})\".format(file.name.split(\".pickle\")[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1fc525-8407-4852-abc4-dba691376337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.timeSeriesParams object at 0x7f9804a43670>, <__main__.timeSeriesParams object at 0x7f9804a43910>, <__main__.timeSeriesParams object at 0x7f9804a41420>, <__main__.timeSeriesParams object at 0x7f9804a407f0>, <__main__.timeSeriesParams object at 0x7f9804a437c0>, <__main__.timeSeriesParams object at 0x7f9804a43e50>, <__main__.timeSeriesParams object at 0x7f9804a42f80>, <__main__.timeSeriesParams object at 0x7f9804a40bb0>, <__main__.timeSeriesParams object at 0x7f9804a439a0>, <__main__.timeSeriesParams object at 0x7f9804a42d10>, <__main__.timeSeriesParams object at 0x7f980ac645e0>, <__main__.timeSeriesParams object at 0x7f980ac67220>, <__main__.timeSeriesParams object at 0x7f980ac64670>]\n",
      "3_KB88r_speculoos_perfect.txt\n",
      "4_rossler_y_speculoos_perfect.txt\n",
      "1_gaussianProcess_speculoos_perfect.txt\n",
      "5_transformed_rossler_z_speculoos_perfect.txt\n",
      "0_gaussianNoise_speculoos_perfect.txt\n",
      "4_rossler_x_speculoos_perfect.txt\n",
      "6_lorenz_z_speculoos_perfect.txt\n",
      "5_transformed_rossler_y_speculoos_perfect.txt\n",
      "6_lorenz_x_speculoos_perfect.txt\n",
      "2_simplePeriodic_speculoos_perfect.txt\n",
      "4_rossler_z_speculoos_perfect.txt\n",
      "6_lorenz_y_speculoos_perfect.txt\n",
      "5_transformed_rossler_x_speculoos_perfect.txt\n"
     ]
    }
   ],
   "source": [
    "print(list_of_tsps)\n",
    "for tsp in list_of_tsps:\n",
    "    print(tsp.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6d704-1bc2-4577-8368-f0ab41592ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d775d41-e5e4-41dd-a892-df8d87489f52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_transformed_rossler_x_speculoos_perfect.txt\n",
      "perfect tauIdx is 73, noisy tauIdx is 72\n",
      "sat_m is 3\n",
      "noisy_sat_m is 6\n",
      "original time series length is 44119\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:164: RuntimeWarning: divide by zero encountered in divide\n",
      "  C0, C1, C2, nArr = sp.Cq(rArr, ts, tau=tsp.bestTauIdx, m=m, theilerWindow=tsp.bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:164: RuntimeWarning: divide by zero encountered in log\n",
      "  C0, C1, C2, nArr = sp.Cq(rArr, ts, tau=tsp.bestTauIdx, m=m, theilerWindow=tsp.bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:165: RuntimeWarning: divide by zero encountered in divide\n",
      "  noisy_C0, noisy_C1, noisy_C2, noisy_nArr = sp.Cq(rArr, noisy_ts, tau=tsp.noisy_bestTauIdx, m=m, theilerWindow=tsp.noisy_bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:165: RuntimeWarning: divide by zero encountered in log\n",
      "  noisy_C0, noisy_C1, noisy_C2, noisy_nArr = sp.Cq(rArr, noisy_ts, tau=tsp.noisy_bestTauIdx, m=m, theilerWindow=tsp.noisy_bestTauIdx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cq call took 23.65 minutes\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:164: RuntimeWarning: divide by zero encountered in divide\n",
      "  C0, C1, C2, nArr = sp.Cq(rArr, ts, tau=tsp.bestTauIdx, m=m, theilerWindow=tsp.bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:164: RuntimeWarning: divide by zero encountered in log\n",
      "  C0, C1, C2, nArr = sp.Cq(rArr, ts, tau=tsp.bestTauIdx, m=m, theilerWindow=tsp.bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:165: RuntimeWarning: divide by zero encountered in divide\n",
      "  noisy_C0, noisy_C1, noisy_C2, noisy_nArr = sp.Cq(rArr, noisy_ts, tau=tsp.noisy_bestTauIdx, m=m, theilerWindow=tsp.noisy_bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:165: RuntimeWarning: divide by zero encountered in log\n",
      "  noisy_C0, noisy_C1, noisy_C2, noisy_nArr = sp.Cq(rArr, noisy_ts, tau=tsp.noisy_bestTauIdx, m=m, theilerWindow=tsp.noisy_bestTauIdx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cq call took 23.56 minutes\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:164: RuntimeWarning: divide by zero encountered in divide\n",
      "  C0, C1, C2, nArr = sp.Cq(rArr, ts, tau=tsp.bestTauIdx, m=m, theilerWindow=tsp.bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:164: RuntimeWarning: divide by zero encountered in log\n",
      "  C0, C1, C2, nArr = sp.Cq(rArr, ts, tau=tsp.bestTauIdx, m=m, theilerWindow=tsp.bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:165: RuntimeWarning: divide by zero encountered in divide\n",
      "  noisy_C0, noisy_C1, noisy_C2, noisy_nArr = sp.Cq(rArr, noisy_ts, tau=tsp.noisy_bestTauIdx, m=m, theilerWindow=tsp.noisy_bestTauIdx)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_1134/3898272196.py:165: RuntimeWarning: divide by zero encountered in log\n",
      "  noisy_C0, noisy_C1, noisy_C2, noisy_nArr = sp.Cq(rArr, noisy_ts, tau=tsp.noisy_bestTauIdx, m=m, theilerWindow=tsp.noisy_bestTauIdx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cq call took 21.23 minutes\n"
     ]
    }
   ],
   "source": [
    "for tsp in [list_of_tsps[-1]]:\n",
    "    print(tsp.filename)\n",
    "    data = np.genfromtxt(tsp.filepath)\n",
    "\n",
    "    if \"0\" in tsp.filename:\n",
    "        ts = data[:,2]\n",
    "        noisy_ts = data[:,2]\n",
    "    else:\n",
    "        ts = data[:,1]\n",
    "        noisy_ts = data[:,2]\n",
    "\n",
    "    print(\"perfect tauIdx is {0}, noisy tauIdx is {1}\".format(tsp.bestTauIdx, tsp.noisy_bestTauIdx))\n",
    "    \n",
    "    '''\n",
    "    #plot the time series\n",
    "    fig, axes = plt.subplots(3,1,figsize=(18,12))\n",
    "    axes[0].plot(data[:,0],ts,marker='.',ms=5,ls='-',color='k',mec='None',lw=0.5,alpha=0.1)\n",
    "    axes[1].errorbar(data[:,0],noisy_ts,yerr=data[:,3],marker='.',ms=5,ls='-',color='k',mec='None',capsize=0.,elinewidth=0.5,lw=0.5,alpha=0.1)\n",
    "    axes[2].errorbar(data[:,0],ts-noisy_ts,yerr=data[:,3],marker='.',ms=5,ls='-',color='k',mec='None',capsize=0.,elinewidth=0.5,lw=0.5,alpha=0.1)\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(data[:,0][0]-0.5, data[:,0][-1]+0.5)\n",
    "\n",
    "    axes[0].set_ylabel(r\"perfect x(t)\",fontsize=14)\n",
    "    axes[1].set_ylabel(r\"noisy x(t)\",fontsize=14)\n",
    "    axes[2].set_ylabel(r\"residuals\",fontsize=14)\n",
    "    axes[2].set_xlabel(r\"$t$ [days]\",fontsize=14)\n",
    "    plt.savefig(\"./timeseriesPlots/{0}_timeseriesplot.png\".format(tsp.filename.split(\".txt\")[0]),bbox_inches=\"tight\")\n",
    "    '''\n",
    "    \"\"\"        \n",
    "    #plot x(t+tau) vs x(t) \n",
    "    fig, axes = plt.subplots(1,2,sharex=True,sharey=True,figsize=(12,6))\n",
    "    axes[0].plot(ts[:-tsp.bestTauIdx], ts[tsp.bestTauIdx:],marker='.',ms=5,ls='-',color='k',mec='None',lw=0.5,alpha=0.1)\n",
    "    axes[1].errorbar(noisy_ts[:-tsp.noisy_bestTauIdx], noisy_ts[tsp.noisy_bestTauIdx:],xerr=data[:,3][:-tsp.noisy_bestTauIdx],yerr=data[:,3][tsp.noisy_bestTauIdx:],marker='.',ms=5,ls='-',color='k',mec='None',capsize=0.,elinewidth=0.5,lw=0.5,alpha=0.01)\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(r\"$x(t)$\",fontsize=14)\n",
    "        ax.set_ylabel(r\"$x(t+\\tau)$\",fontsize=14)\n",
    "        ax.set_aspect(\"equal\")\n",
    "    plt.savefig(\"./delayPlots/{0}_delayplot.png\".format(tsp.filename.split(\".txt\")[0]),bbox_inches=\"tight\")\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"sat_m is {0}\".format(tsp.sat_m))\n",
    "    print(\"noisy_sat_m is {0}\".format(tsp.noisy_sat_m))\n",
    "\n",
    "    \n",
    "    if tsp.sat_m is None:\n",
    "        tsp.sat_m = 10\n",
    "    if tsp.noisy_sat_m is None:\n",
    "        tsp.noisy_sat_m = 10\n",
    "\n",
    "    #ts = ts[:10000]\n",
    "    #noisy_ts = noisy_ts[:10000]\n",
    "    '''\n",
    "    stp = tiseanio(\"stp\",\n",
    "                    '-d',tsp.bestTauIdx,\n",
    "                    '-m',tsp.sat_m,\n",
    "                    '-c',1,\n",
    "                    '-t',500,\n",
    "                    data=ts)\n",
    "    stp = stp[0]\n",
    "\n",
    "    noisy_stp = tiseanio(\"stp\",\n",
    "                    '-d',tsp.noisy_bestTauIdx,\n",
    "                    '-m',tsp.noisy_sat_m,\n",
    "                    '-c',1,\n",
    "                    '-t',500,\n",
    "                    data=noisy_ts)\n",
    "    noisy_stp = noisy_stp[0]\n",
    "\n",
    "    delayMat = sp.delayMatrix(ts, tau=tsp.bestTauIdx, m=tsp.sat_m)\n",
    "    noisy_delayMat = sp.delayMatrix(noisy_ts, tau=tsp.noisy_bestTauIdx, m=tsp.noisy_sat_m)\n",
    "   \n",
    "    ball_tree = BallTree(delayMat, leaf_size=10, metric='euclidean')\n",
    "    noisy_ball_tree = BallTree(noisy_delayMat, leaf_size=10, metric='euclidean')\n",
    "\n",
    "    (distances,indices) = ball_tree.query(delayMat, k=np.shape(delayMat)[0],return_distance=True)\n",
    "\n",
    "    distances = np.ravel(distances[:,1:])\n",
    "    indices = indices - np.atleast_2d(np.arange(np.shape(delayMat)[0])).T\n",
    "    indices = np.ravel(indices[:,1:])\n",
    "\n",
    "    # we don't want to plot each pair of points twice, so limit ourselves to the positive indices\n",
    "    distances = distances[indices >= 0]\n",
    "    indices = indices[indices >= 0]\n",
    "    \n",
    "    (noisy_distances, noisy_indices) = noisy_ball_tree.query(noisy_delayMat, k=np.shape(noisy_delayMat)[0],return_distance=True)\n",
    "    noisy_distances = np.ravel(noisy_distances[:,1:])\n",
    "    noisy_indices = noisy_indices - np.atleast_2d(np.arange(np.shape(noisy_delayMat)[0])).T\n",
    "    noisy_indices = np.ravel(noisy_indices[:,1:])\n",
    "    noisy_distances = noisy_distances[noisy_indices >= 0]\n",
    "    noisy_indices = noisy_indices[noisy_indices >= 0]\n",
    "    \n",
    "    #make spatial separation vs time separation plots, cf Provenzale et al 1992\n",
    "    fig, axes = plt.subplots(2,2,sharex=True,sharey=True,figsize=(16,12))\n",
    "    axes[0,0].plot(indices, distances, 'k.', ms=1,alpha=0.01)\n",
    "    axes[0,1].plot(noisy_indices, noisy_distances, 'k.',ms=1,alpha=0.01)\n",
    "\n",
    "    axes[1,0].plot(stp[:,0], stp[:,1],color='k',marker='.',linestyle='None',ms=1)\n",
    "    axes[1,1].plot(noisy_stp[:,0], noisy_stp[:,1],color='k',marker='.',linestyle='None',ms=1)\n",
    "\n",
    "    \n",
    "    axes[0,0].axvline(tsp.bestTauIdx,color='b',lw=2,label=\"best tau = {0}\".format(tsp.bestTauIdx))\n",
    "    axes[1,0].axvline(tsp.bestTauIdx,color='b',lw=2)\n",
    "\n",
    "    axes[0,1].axvline(tsp.noisy_bestTauIdx,color='b',lw=2,label=\"best tau = {0}\".format(tsp.noisy_bestTauIdx))\n",
    "    axes[1,1].axvline(tsp.noisy_bestTauIdx,color='b',lw=2)\n",
    "\n",
    "    axes[0,0].axvline(10*tsp.bestTauIdx,color='r',lw=2,label=\"10*best tau\")\n",
    "    axes[1,0].axvline(10*tsp.bestTauIdx,color='r',lw=2)\n",
    "\n",
    "    axes[0,1].axvline(10*tsp.noisy_bestTauIdx,color='r',lw=2,label=\"10*best tau\")\n",
    "    axes[1,1].axvline(10*tsp.noisy_bestTauIdx,color='r',lw=2)\n",
    "\n",
    "    axes[0,0].legend(loc=\"lower right\",fontsize=12)\n",
    "    axes[0,1].legend(loc=\"lower right\",fontsize=12)\n",
    "    for ax in np.ravel(axes):\n",
    "        ax.set_xlabel(r\"separation in time [cadences]\",fontsize=12)\n",
    "        ax.set_ylabel(r\"separation in space [units of time series]\",fontsize=12)\n",
    "        ax.set_xlim(0,np.max(indices)/3)\n",
    "        #ax.set_yscale(\"log\")\n",
    "\n",
    "    axes[0,0].set_title(\"perfect, my code\")\n",
    "    axes[0,1].set_title(\"noisy, my code\")\n",
    "    axes[1,0].set_title(\"perfect, TISEAN\")\n",
    "    axes[1,1].set_title(\"noisy, TISEAN\")\n",
    "    \n",
    "    plt.savefig(\"./spacetimesepPlots/{0}_spacetimesep.png\".format(tsp.filename.split(\".txt\")[0]),bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    '''\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"original time series length is {0}\".format(len(ts)))\n",
    "    #ts = ts[:10000]\n",
    "    #noisy_ts = noisy_ts[:10000]\n",
    "\n",
    "            \n",
    "    # scale ts to be between 0 and 1\n",
    "    ts = (ts - np.min(ts))/np.ptp(ts)\n",
    "    noisy_ts = (noisy_ts - np.min(noisy_ts))/np.ptp(noisy_ts)\n",
    "    '''\n",
    "    # just to see how things scale, run all the way up to Mmax=10\n",
    "    start = time.time()\n",
    "    d2dict = sp.d2_tisean(timeSeries=ts,tau=tsp.bestTauIdx,m=10,thelier=tsp.bestTauIdx)\n",
    "    firstd2call = time.time()\n",
    "    noisy_d2dict = sp.d2_tisean(timeSeries=noisy_ts,tau=tsp.noisy_bestTauIdx,m=10,thelier=tsp.noisy_bestTauIdx)\n",
    "    noisyd2call = time.time()\n",
    "\n",
    "    print(\"first d2 call took {0} seconds\".format(np.round(firstd2call-start,2)))\n",
    "    print(\"noisy d2 call took {0} seconds\".format(np.round(noisyd2call-firstd2call,2)))\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    min_r = 1.e-3\n",
    "    max_r = 1\n",
    "    logrArr = np.linspace(np.log10(min_r),np.log10(max_r),100)\n",
    "    rArr = 10**logrArr\n",
    "\n",
    "    for m in range(8,11):\n",
    "        print(m)\n",
    "\n",
    "        start_cq = time.time()\n",
    "        \n",
    "        C0, C1, C2, nArr = sp.Cq(rArr, ts, tau=tsp.bestTauIdx, m=m, theilerWindow=tsp.bestTauIdx)\n",
    "        noisy_C0, noisy_C1, noisy_C2, noisy_nArr = sp.Cq(rArr, noisy_ts, tau=tsp.noisy_bestTauIdx, m=m, theilerWindow=tsp.noisy_bestTauIdx)\n",
    "    \n",
    "        end_cq = time.time()\n",
    "    \n",
    "        print(\"Cq call took {0} minutes\".format(np.round((end_cq - start_cq)/60, 2)))\n",
    "        \n",
    "        #np.save(\"./c2arrs/{0}_tisean_c2_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]), d2dict[\"c2\"])\n",
    "        #np.save(\"./c2arrs/{0}_tisean_d2_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]), d2dict[\"d2\"])\n",
    "        #np.save(\"./c2arrs/{0}_tisean_h2_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]), d2dict[\"h2\"])\n",
    "    \n",
    "        #np.save(\"./c2arrs/{0}_noisy_tisean_c2_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]), noisy_d2dict[\"c2\"])\n",
    "        #np.save(\"./c2arrs/{0}_noisy_tisean_d2_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]), noisy_d2dict[\"d2\"])\n",
    "        #np.save(\"./c2arrs/{0}_noisy_tisean_h2_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]), noisy_d2dict[\"h2\"])\n",
    "    \n",
    "        np.save(\"./c2arrs/{0}_c0_norm0to1_m={1}.npy\".format(tsp.filename.split(\".txt\")[0],m), C0)\n",
    "        np.save(\"./c2arrs/{0}_noisy_c0_norm0to1_m={1}.npy\".format(tsp.filename.split(\".txt\")[0],m), noisy_C0)\n",
    "    \n",
    "        np.save(\"./c2arrs/{0}_c1_norm0to1_m={1}.npy\".format(tsp.filename.split(\".txt\")[0],m), C1)\n",
    "        np.save(\"./c2arrs/{0}_noisy_c1_norm0to1_m={1}.npy\".format(tsp.filename.split(\".txt\")[0],m), noisy_C1)\n",
    "    \n",
    "        np.save(\"./c2arrs/{0}_c2_norm0to1_m={1}.npy\".format(tsp.filename.split(\".txt\")[0],m), C2)\n",
    "        np.save(\"./c2arrs/{0}_noisy_c2_norm0to1_m={1}.npy\".format(tsp.filename.split(\".txt\")[0],m), noisy_C2)\n",
    "    \n",
    "        np.save(\"./c2arrs/{0}_nArr_norm0to1_m={1}.npy\".format(tsp.filename.split(\".txt\")[0],m), nArr)\n",
    "        np.save(\"./c2arrs/{0}_noisy_nArr_norm0to1_m={1}.npy\".format(tsp.filename.split(\".txt\")[0],m), noisy_nArr)\n",
    "    '''\n",
    "    \n",
    "    tisean_c2 = np.load(\"./c2arrs/{0}_tisean_c2_short_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "    tisean_d2 = np.load(\"./c2arrs/{0}_tisean_d2_short_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "    tisean_h2 = np.load(\"./c2arrs/{0}_tisean_h2_short_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "\n",
    "    noisy_tisean_c2 = np.load(\"./c2arrs/{0}_noisy_tisean_c2_short_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "    noisy_tisean_d2 = np.load(\"./c2arrs/{0}_noisy_tisean_d2_short_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "    noisy_tisean_h2 = np.load(\"./c2arrs/{0}_noisy_tisean_h2_short_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "\n",
    "    C2 = np.load(\"./c2arrs/{0}_c2_short_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "    noisy_C2 = np.load(\"./c2arrs/{0}_noisy_c2_short_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "\n",
    "    \n",
    "    #print(np.shape(tisean_c2))\n",
    "    #print(np.shape(tisean_d2))\n",
    "\n",
    "    D2 = (np.log10(C2[1:]) - np.log10(C2[0:-1]))/(logrArr[1] - logrArr[0])\n",
    "    noisy_D2 = (np.log10(noisy_C2[1:]) - np.log10(noisy_C2[0:-1]))/(logrArr[1] - logrArr[0])\n",
    "    logrArr_int = (logrArr[0:-1] + logrArr[1:])/2.\n",
    "    rArr_int = 10**logrArr_int\n",
    "\n",
    "    #plt.plot(tisean_d2[1:,0] - tisean_d2[0:-1,0],'k.')\n",
    "    #plt.show()\n",
    "\n",
    "    # in order to plot d2 corresponding to the saturated value of m more clearly\n",
    "    diff = (tisean_d2[1:,0] - tisean_d2[0:-1,0])\n",
    "    diff_mask = (diff > 0.5)\n",
    "    noisy_diff = (noisy_tisean_d2[1:,0] - noisy_tisean_d2[0:-1,0])\n",
    "    noisy_diff_mask = (noisy_diff > 0.5)\n",
    "\n",
    "    d2_m_switch_ind = np.arange(len(tisean_d2[1:,0]))[diff_mask]\n",
    "    d2_m_switch_ind = np.concatenate((np.atleast_1d(np.array((0))), d2_m_switch_ind, np.atleast_1d(np.array((len(tisean_d2[:,0]))))))\n",
    "    noisy_d2_m_switch_ind = np.arange(len(noisy_tisean_d2[1:,0]))[noisy_diff_mask] \n",
    "    noisy_d2_m_switch_ind = np.concatenate((np.atleast_1d(np.array((0))), noisy_d2_m_switch_ind,np.atleast_1d(np.array((len(noisy_tisean_d2[:,0]))))))\n",
    "\n",
    "\n",
    "\n",
    "    # \"true\" C2 values from Sprott & Rowlands 2000\n",
    "    lorenz_C2_lower = 2.049-0.096\n",
    "    lorenz_C2_upper = 2.049+0.096\n",
    "    lorenz_KY = 2.062\n",
    "    rossler_C2_lower = 1.986-0.078\n",
    "    rossler_C2_upper = 1.986+0.078\n",
    "    rossler_KY = 2.013\n",
    "    \n",
    "    fig, axes = plt.subplots(2,2,figsize=(12,8))\n",
    "    axes[0,0].scatter(tisean_c2[:,0], tisean_c2[:,1],s=0.1,c='k',label=\"TISEAN Chebyshev c2, all m\")\n",
    "    axes[0,0].scatter(tisean_c2[(tsp.sat_m-1)*100:tsp.sat_m*100,0], tisean_c2[(tsp.sat_m-1)*100:tsp.sat_m*100,1],s=0.2,c='r',label=\"TISEAN Chebyshev c2, sat_m\")\n",
    "    axes[0,0].plot(rArr, C2, 'b.',ms=2,ls='None',label=\"my Euclidean c2\")\n",
    "    axes[0,0].set_yscale('log')\n",
    "    axes[0,0].set_ylabel(\"correlation sum\")\n",
    "    axes[0,0].legend(loc=\"lower right\",fontsize=10)\n",
    "\n",
    "    axes[0,1].scatter(tisean_d2[:,0], tisean_d2[:,1],s=0.1,c='k')\n",
    "    axes[0,1].scatter(tisean_d2[d2_m_switch_ind[tsp.sat_m-1]:d2_m_switch_ind[tsp.sat_m],0],tisean_d2[d2_m_switch_ind[tsp.sat_m-1]:d2_m_switch_ind[tsp.sat_m],1],s=0.2,c='r')\n",
    "    axes[0,1].plot(rArr_int, D2, 'b.',ms=2,ls='None')\n",
    "    axes[0,1].set_ylabel(\"correlation dimension\")\n",
    "\n",
    "    #axes[0,2].scatter(tisean_h2[:,0], tisean_h2[:,1],s=0.1,c='k')\n",
    "    #axes[0,2].set_ylabel(\"correlation entropy\")\n",
    "\n",
    "\n",
    "    axes[1,0].scatter(noisy_tisean_c2[:,0], noisy_tisean_c2[:,1],s=0.1,c='k')\n",
    "    axes[1,0].scatter(noisy_tisean_c2[(tsp.noisy_sat_m-1)*100:tsp.noisy_sat_m*100,0], noisy_tisean_c2[(tsp.noisy_sat_m-1)*100:tsp.noisy_sat_m*100,1],s=0.2,c='r')\n",
    "    axes[1,0].plot(rArr, noisy_C2, 'b.',ms=2,ls='None')\n",
    "    axes[1,0].set_yscale('log')\n",
    "    axes[1,0].set_ylabel(\"correlation sum\")\n",
    "\n",
    "    axes[1,1].scatter(noisy_tisean_d2[:,0], noisy_tisean_d2[:,1],s=0.1,c='k')\n",
    "    axes[1,1].scatter(noisy_tisean_d2[noisy_d2_m_switch_ind[tsp.noisy_sat_m-1]:noisy_d2_m_switch_ind[tsp.noisy_sat_m],0],noisy_tisean_d2[noisy_d2_m_switch_ind[tsp.noisy_sat_m-1]:noisy_d2_m_switch_ind[tsp.noisy_sat_m],1],s=0.2,c='r')\n",
    "    axes[1,1].plot(rArr_int, noisy_D2, 'b.',ms=2,ls='None')\n",
    "    axes[1,1].set_ylabel(\"correlation dimension\")\n",
    "\n",
    "    #axes[1,2].scatter(noisy_tisean_h2[:,0], noisy_tisean_h2[:,1],s=0.1,c='k')\n",
    "    #axes[1,2].set_ylabel(\"correlation entropy\")\n",
    "\n",
    "    if \"4\" in tsp.filename or \"5\" in tsp.filename:\n",
    "        for ax in axes[:,1]:\n",
    "            ax.axhline(rossler_KY, color='k',lw=0.5)\n",
    "            ax.fill_between(np.linspace(7.e-4, 1.5, 3), y1=rossler_C2_lower*np.ones(3),y2=rossler_C2_upper*np.ones(3), color='k',alpha=0.1)\n",
    "    elif \"6\" in tsp.filename:\n",
    "        for ax in axes[:,1]:\n",
    "            ax.axhline(lorenz_KY, color='k',lw=0.5)\n",
    "            ax.fill_between(np.linspace(7.e-4, 1.5, 3), y1=lorenz_C2_lower*np.ones(3),y2=lorenz_C2_upper*np.ones(3), color='k',alpha=0.1)\n",
    "            \n",
    "    for ax in np.ravel(axes):\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlim(7.e-4, 1.5)\n",
    "\n",
    "    for ax in axes[1]:\n",
    "        ax.set_xlabel('length scale')\n",
    "\n",
    "    for ax in axes[:,0]:\n",
    "        ax.set_ylim(np.min(tisean_c2[:,1]),1.5)\n",
    "\n",
    "    for ax in axes[:,1]:\n",
    "        ax.set_ylim(-0.5, 8)\n",
    "\n",
    "    #for ax in axes[:,2]:\n",
    "    #    ax.set_ylim(-0.5,6.5)\n",
    "\n",
    "    axes[0,0].set_title(\"sat_m = {0}\".format(tsp.sat_m))\n",
    "    axes[1,0].set_title(\"noisy sat_m = {0}\".format(tsp.noisy_sat_m))\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    #plt.show()\n",
    "    plt.savefig(\"./c2Plots/{0}_short_norm0to1_Cq_comp.png\".format(tsp.filename.split(\".txt\")[0]),bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fef325-2606-4c14-a179-84c0fe4819d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "      \n",
    "    \"\"\"\n",
    "    mMax = 8\n",
    "            \n",
    "    if tsp.sat_m is None:\n",
    "        print(\"no saturation\")\n",
    "        mArr = np.arange(2, mMax+1)\n",
    "    else:\n",
    "        if tsp.sat_m + 1 <= mMax:\n",
    "            mArr = np.array((tsp.sat_m, tsp.sat_m + 1))\n",
    "        elif tsp.sat_m <= mMax:\n",
    "            mArr = np.array((tsp.sat_m))\n",
    "        else:\n",
    "            mArr = np.arange(2, mMax+1)\n",
    "    \"\"\"\n",
    "    '''\n",
    "    print(mArr)\n",
    "            \n",
    "    for m in mArr:\n",
    "        print(m)\n",
    "        if os.path.exists(\"./{0}_tau={1}_m={2}_C0.npy\".format(tsp.filename.split(\".txt\")[0],tsp.bestTauIdx,m)):\n",
    "            pass\n",
    "        else:\n",
    "            C0_, C1_, C2_, nArr_ = sp.Cq(rArr=rArr, timeSeries=ts, tau = tsp.bestTauIdx, m = m)\n",
    "            np.save(\"./{0}_tau={1}_m={2}_C0.npy\".format(tsp.filename.split(\".txt\")[0],tsp.bestTauIdx,m),C0_)\n",
    "            np.save(\"./{0}_tau={1}_m={2}_C1.npy\".format(tsp.filename.split(\".txt\")[0],tsp.bestTauIdx,m),C1_)\n",
    "            np.save(\"./{0}_tau={1}_m={2}_C2.npy\".format(tsp.filename.split(\".txt\")[0],tsp.bestTauIdx,m),C2_)\n",
    "            np.save(\"./{0}_tau={1}_m={2}_nArr.npy\".format(tsp.filename.split(\".txt\")[0],tsp.bestTauIdx,m),nArr_)\n",
    "\n",
    "            delayMat = sp.delayMatrix(ts, tsp.bestTauIdx, m)\n",
    "                    \n",
    "            N = np.shape(nArr_)[0]\n",
    "            medians = np.percentile(nArr_, 50, axis=0)\n",
    "                        \n",
    "            # exclude values of r where the median of n(r) is <= 10./N . Cutoff is a little arbitrary but the idea is that these points don't have enough neighbors.\n",
    "            enoughNeighborsIdxs = np.arange(len(rArr))[medians > 10./N]\n",
    "            firstGood = enoughNeighborsIdxs[0]\n",
    "                   \n",
    "            exclude values of r where any n(r) are NaN. The time series is not long enough to populate all the neighbors of the points.\n",
    "            anyNans = [np.any(~np.isfinite(nArr_[:,i])) for i in range(len(rArr))]\n",
    "            anyNans = np.array(anyNans)\n",
    "            nansIdxs = np.arange(len(rArr))[anyNans]\n",
    "            lastGood = nansIdxs[0]\n",
    "                    \n",
    "            params_C2, params_unc_C2 = sp.fitLinearRegime(rArr, nArr_, C2_)\n",
    "                    \n",
    "            params_dist, params_dist_1sigma = sp.powerLawSlopeDistribution(rArr, nArr_)\n",
    "\n",
    "            fig, axes = plt.subplots(m,1,figsize=(6, 6*(m-1)))\n",
    "            axes = np.atleast_1d(axes)\n",
    "            for j in range(1,m):\n",
    "                axes[m-1-j].plot(delayMat[:,0],delayMat[:,j],linestyle='-',color='k',lw=0.25,marker=\"None\",zorder=1)\n",
    "                im = axes[m-1-j].scatter(delayMat[:,0],delayMat[:,j],c=params_dist[:,0],s=10,cmap=\"magma\",linewidths=0,alpha=0.9,zorder=2)\n",
    "                        \n",
    "                divider = make_axes_locatable(axes[m-1-j])\n",
    "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                cb = plt.colorbar(im, cax=cax)\n",
    "                cb.set_label(label=r\"Power law slope fit to $n(x_i)$ vs. $r$\",fontsize=14)\n",
    "                        \n",
    "                axes[m-1-j].set_ylabel(r\"$x_i + {0}\\tau$\".format(j),fontsize=14)\n",
    "                    \n",
    "            axes[m-2].set_xlabel(r\"$x_i$\",fontsize=14)\n",
    "                    \n",
    "            im = axes[m-1].scatter(np.arange(0,len(delayMat[:,0])), delayMat[:,0], c=params_dist[:,0],s=10,cmap=\"magma\")\n",
    "            divider = make_axes_locatable(axes[m-1])\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cb = plt.colorbar(im, cax=cax)\n",
    "            cb.set_label(label=r\"Power law slope fit to $n(x_i)$ vs. $r$\",fontsize=14)\n",
    "            axes[m-1].set_ylabel(r\"$x_i$\",fontsize=14)\n",
    "            axes[m-1].set_xlabel(\"i\", fontsize=14)\n",
    "                    \n",
    "            n_t = len(delayMat[:,0])//tsp.bestTauIdx\n",
    "                    \n",
    "            #for k in range(n_t+1):\n",
    "            #    axes[m-1].axvline(k*tsp.bestTauIdx, color='k', ls=\"-\",lw=0.5)\n",
    "                    \n",
    "            plt.subplots_adjust(hspace=0.2)\n",
    "            #plt.show()\n",
    "            plt.savefig(\"./{0}_m={1}_colormapped_C2.png\".format(tsp.filename.split(\".txt\")[0],m),bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "    \"\"\"\n",
    "    if tsp.noisy_sat_m is None:\n",
    "        print(\"no saturation\")\n",
    "        noisy_mArr = np.arange(2, mMax+1)\n",
    "    else:\n",
    "        if tsp.noisy_sat_m + 1 <= mMax:\n",
    "            noisy_mArr = np.array((tsp.noisy_sat_m, tsp.noisy_sat_m + 1))\n",
    "        elif tsp.noisy_sat_m <= mMax:\n",
    "            tsp.noisy_mArr = np.array((tsp.noisy_sat_m))\n",
    "        else:\n",
    "            tsp.noisy_mArr = np.arange(2, mMax+1)\n",
    "    \"\"\"\n",
    "    print(noisy_mArr)\n",
    "    \n",
    "    for m in noisy_mArr:\n",
    "        print(m)\n",
    "        if os.path.exists(\"./{0}_tau={1}_m={2}_C0.npy\".format(tsp.filename.split(\".txt\")[0],tsp.noisy_bestTauIdx,m)):\n",
    "             pass\n",
    "        else:\n",
    "            C0_, C1_, C2_, nArr_ = sp.Cq(rArr=rArr, timeSeries=noisy_ts, tau = tsp.noisy_bestTauIdx, m = m)\n",
    "            np.save(\"./{0}_tau={1}_m={2}_noisy_C0.npy\".format(tsp.filename.split(\".txt\")[0],tsp.noisy_bestTauIdx,m),C0_)\n",
    "            np.save(\"./{0}_tau={1}_m={2}_noisy_C1.npy\".format(tsp.filename.split(\".txt\")[0],tsp.noisy_bestTauIdx,m),C1_)\n",
    "            np.save(\"./{0}_tau={1}_m={2}_noisy_C2.npy\".format(tsp.filename.split(\".txt\")[0],tsp.noisy_bestTauIdx,m),C2_)\n",
    "            np.save(\"./{0}_tau={1}_m={2}_noisy_nArr.npy\".format(tsp.filename.split(\".txt\")[0],tsp.noisy_bestTauIdx,m),nArr_)\n",
    "\n",
    "            delayMat = sp.delayMatrix(noisy_ts, tsp.noisy_bestTauIdx, m)\n",
    "                    \n",
    "            N = np.shape(nArr_)[0]\n",
    "            medians = np.percentile(nArr_, 50, axis=0)\n",
    "                        \n",
    "            # exclude values of r where the median of n(r) is <= 10./N . Cutoff is a little arbitrary but the idea is that these points don't have enough neighbors.\n",
    "            enoughNeighborsIdxs = np.arange(len(rArr))[medians > 10./N]\n",
    "            firstGood = enoughNeighborsIdxs[0]\n",
    "                    \n",
    "            # exclude values of r where any n(r) are NaN. The time series is not long enough to populate all the neighbors of the points.\n",
    "            anyNans = [np.any(~np.isfinite(nArr_[:,i])) for i in range(len(rArr))]\n",
    "            anyNans = np.array(anyNans)\n",
    "            nansIdxs = np.arange(len(rArr))[anyNans]\n",
    "            lastGood = nansIdxs[0]\n",
    "                    \n",
    "            params_C2, params_unc_C2 = sp.fitLinearRegime(rArr, nArr_, C2_)\n",
    "                    \n",
    "            params_dist, params_dist_1sigma = sp.powerLawSlopeDistribution(rArr, nArr_)\n",
    "\n",
    "            fig, axes = plt.subplots(m,1,figsize=(6, 6*(m-1)))\n",
    "            axes = np.atleast_1d(axes)\n",
    "            for j in range(1,m):\n",
    "                axes[m-1-j].plot(delayMat[:,0],delayMat[:,j],linestyle='-',color='k',lw=0.25,marker=\"None\",zorder=1)\n",
    "                im = axes[m-1-j].scatter(delayMat[:,0],delayMat[:,j],c=params_dist[:,0],s=10,cmap=\"magma\",linewidths=0,alpha=0.9,zorder=2)\n",
    "                        \n",
    "                divider = make_axes_locatable(axes[m-1-j])\n",
    "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "                cb = plt.colorbar(im, cax=cax)\n",
    "                cb.set_label(label=r\"Power law slope fit to $n(x_i)$ vs. $r$\",fontsize=14)\n",
    "                        \n",
    "                axes[m-1-j].set_ylabel(r\"$x_i + {0}\\tau$\".format(j),fontsize=14)\n",
    "                    \n",
    "            axes[m-2].set_xlabel(r\"$x_i$\",fontsize=14)\n",
    "                    \n",
    "            im = axes[m-1].scatter(np.arange(0,len(delayMat[:,0])), delayMat[:,0], c=params_dist[:,0],s=10,cmap=\"magma\")\n",
    "            divider = make_axes_locatable(axes[m-1])\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cb = plt.colorbar(im, cax=cax)\n",
    "            cb.set_label(label=r\"Power law slope fit to $n(x_i)$ vs. $r$\",fontsize=14)\n",
    "            axes[m-1].set_ylabel(r\"$x_i$\",fontsize=14)\n",
    "            axes[m-1].set_xlabel(\"i\", fontsize=14)\n",
    "                    \n",
    "            n_t = len(delayMat[:,0])//tsp.noisy_bestTauIdx\n",
    "                    \n",
    "            #for k in range(n_t+1):\n",
    "            #    axes[m-1].axvline(k*tsp.noisy_bestTauIdx, color='k', ls=\"-\",lw=0.5)\n",
    "                    \n",
    "            plt.subplots_adjust(hspace=0.2)\n",
    "            #plt.show()\n",
    "            plt.savefig(\"./{0}_m={1}_colormapped_C2_noisy.png\".format(tsp.filename.split(\".txt\")[0],m),bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    '''\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dcc443-beae-40e4-92c3-a1405ce8e22e",
   "metadata": {},
   "source": [
    "# Colormap delay diagrams by recovered dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f5d4640-05fe-4804-9a45-43f3c324ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_KB88r_speculoos_perfect.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_37088/161106567.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  params_C2, params_unc_C2 = sp.fitLinearRegime(rArr, nArr, C2)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_37088/161106567.py:56: RuntimeWarning: divide by zero encountered in log10\n",
      "  params_C2, params_unc_C2 = sp.fitLinearRegime(rArr, nArr, C2)\n",
      "/var/folders/f8/fqd5scnj31n0br62dg8rnm2w0000gn/T/ipykernel_37088/161106567.py:58: RuntimeWarning: divide by zero encountered in log10\n",
      "  params_dist, params_dist_1sigma = sp.powerLawSlopeDistribution(rArr, nArr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_rossler_y_speculoos_perfect.txt\n",
      "1_gaussianProcess_speculoos_perfect.txt\n",
      "5_transformed_rossler_z_speculoos_perfect.txt\n",
      "0_gaussianNoise_speculoos_perfect.txt\n",
      "4_rossler_x_speculoos_perfect.txt\n",
      "6_lorenz_z_speculoos_perfect.txt\n",
      "5_transformed_rossler_y_speculoos_perfect.txt\n",
      "6_lorenz_x_speculoos_perfect.txt\n",
      "2_simplePeriodic_speculoos_perfect.txt\n",
      "4_rossler_z_speculoos_perfect.txt\n",
      "6_lorenz_y_speculoos_perfect.txt\n",
      "5_transformed_rossler_x_speculoos_perfect.txt\n"
     ]
    }
   ],
   "source": [
    "for tsp in list_of_tsps:\n",
    "    print(tsp.filename)\n",
    "    data = np.genfromtxt(tsp.filepath)\n",
    "\n",
    "    if \"0\" in tsp.filename:\n",
    "        ts = data[:,2]\n",
    "        noisy_ts = data[:,2]\n",
    "    else:\n",
    "        ts = data[:,1]\n",
    "        noisy_ts = data[:,2]\n",
    "\n",
    "    #print(\"perfect tauIdx is {0}, noisy tauIdx is {1}\".format(tsp.bestTauIdx, tsp.noisy_bestTauIdx))\n",
    "    #print(\"sat_m is {0}, noisy sat_m is {1}\".format(tsp.sat_m, tsp.noisy_sat_m))\n",
    "\n",
    "    if tsp.sat_m is None:\n",
    "        tsp.sat_m = 10\n",
    "    if tsp.noisy_sat_m is None:\n",
    "        tsp.noisy_sat_m = 10\n",
    "\n",
    "    #print(\"original time series length is {0}\".format(len(ts)))\n",
    "\n",
    "            \n",
    "    # scale ts to be between 0 and 1\n",
    "    #ts = (ts - np.min(ts))/np.ptp(ts)\n",
    "    #noisy_ts = (noisy_ts - np.min(noisy_ts))/np.ptp(noisy_ts)\n",
    "\n",
    "    min_r = 1.e-3\n",
    "    max_r = 1\n",
    "    logrArr = np.linspace(np.log10(min_r),np.log10(max_r),100)\n",
    "    rArr = 10**logrArr\n",
    "    \n",
    "    C2 = np.load(\"./c2arrs/{0}_c2_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "    noisy_C2 = np.load(\"./c2arrs/{0}_noisy_c2_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "\n",
    "    nArr = np.load(\"./c2arrs/{0}_nArr_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "    noisy_nArr = np.load(\"./c2arrs/{0}_noisy_nArr_norm0to1.npy\".format(tsp.filename.split(\".txt\")[0]))\n",
    "\n",
    "    delayMat = sp.delayMatrix(ts, tsp.bestTauIdx, tsp.sat_m)\n",
    "    \n",
    "    N = np.shape(nArr)[0]\n",
    "    medians = np.percentile(nArr, 50, axis=0)\n",
    "\n",
    "    # exclude values of r where the median of n(r) is <= 10./N . Cutoff is a little arbitrary but the idea is that these points don't have enough neighbors.\n",
    "    enoughNeighborsIdxs = np.arange(len(rArr))[medians > 10./N]\n",
    "    firstGood = enoughNeighborsIdxs[0]\n",
    "    \n",
    "    # exclude values of r where any n(r) are NaN. The time series is not long enough to populate all the neighbors of the points.\n",
    "    anyNans = [np.any(~np.isfinite(nArr[:,i])) for i in range(len(rArr))]\n",
    "    anyNans = np.array(anyNans)\n",
    "    nansIdxs = np.arange(len(rArr))[anyNans]\n",
    "    try:\n",
    "        lastGood = nansIdxs[0]\n",
    "    except IndexError:\n",
    "        lastGood = -1\n",
    "    \n",
    "    params_C2, params_unc_C2 = sp.fitLinearRegime(rArr, nArr, C2)\n",
    "    \n",
    "    params_dist, params_dist_1sigma = sp.powerLawSlopeDistribution(rArr, nArr)\n",
    "          \n",
    "    #print(np.shape(params_dist))\n",
    "    #print(np.shape(params_dist[:,0]))\n",
    "    m = tsp.sat_m\n",
    "    tau = tsp.bestTauIdx\n",
    "    \n",
    "    fig, axes = plt.subplots(m,1,figsize=(6, 6*(m-1)))\n",
    "    axes = np.atleast_1d(axes)\n",
    "    for j in range(1,m):\n",
    "        if \"0\" not in tsp.filename:\n",
    "            axes[m-1-j].plot(delayMat[:,0],delayMat[:,j],linestyle='-',color='k',lw=0.25,marker=\"None\",zorder=1)\n",
    "        im = axes[m-1-j].scatter(delayMat[:,0],delayMat[:,j],c=params_dist[:,0],s=2,cmap=\"magma\",linewidths=0,alpha=0.7,zorder=2)\n",
    "        \n",
    "        divider = make_axes_locatable(axes[m-1-j])\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        cb = plt.colorbar(im, cax=cax)\n",
    "        cb.set_label(label=r\"Power law slope fit to $n(x_i)$ vs. $r$\",fontsize=14)\n",
    "        \n",
    "        axes[m-1-j].set_ylabel(r\"$x_i + {0}\\tau$\".format(j),fontsize=14)\n",
    "    \n",
    "    axes[m-2].set_xlabel(r\"$x_i$\",fontsize=14)\n",
    "\n",
    "    if \"0\" not in tsp.filename:\n",
    "        axes[m-1].plot(np.arange(0,len(delayMat[:,0])), delayMat[:,0], 'k-',lw=0.25,zorder=1)\n",
    "    im = axes[m-1].scatter(np.arange(0,len(delayMat[:,0])), delayMat[:,0], c=params_dist[:,0],s=2,cmap=\"magma\",alpha=0.7,zorder=2)\n",
    "    divider = make_axes_locatable(axes[m-1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cb = plt.colorbar(im, cax=cax)\n",
    "    cb.set_label(label=r\"Power law slope fit to $n(x_i)$ vs. $r$\",fontsize=14)\n",
    "    axes[m-1].set_ylabel(r\"$x_i$\",fontsize=14)\n",
    "    axes[m-1].set_xlabel(\"i\", fontsize=14)\n",
    "    \n",
    "    n_t = len(delayMat[:,0])//tau\n",
    "    \n",
    "    #for k in range(n_t+1):\n",
    "    #    axes[m-1].axvline(k*tau, color='k', ls=\"-\",lw=0.5)\n",
    "\n",
    "    for ax in axes[0:-1]:\n",
    "        ax.set_aspect(\"equal\")\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    #plt.show()\n",
    "    plt.savefig(\"./delayPlots/{0}_delayplot_C2colormap.png\".format(tsp.filename.split(\".txt\")[0]),bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584bff73-8d50-4f5b-921f-93ec4bb14b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
